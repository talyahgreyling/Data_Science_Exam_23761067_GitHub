---
title: "Data Science Methods for Economics and Finance 871 Exam" 
author: "Talyah Greyling (23761067)"
output:
  md_document:
    variant: markdown_github
---

# PURPOSE: 

This readme is located in my repository for the Data Science Methods for Economics and Finance 871 Exam. 
The repository contains all of my code, figures, tables, and write-ups for the exam. 
There are a total of 5 questions & each question has its own folder with an accompanying Readme, code and data folder.

# SETUP CREATION: 

```{r create folders, eval = FALSE}

#The '23761067' project was created by copying the file path for my repository file (C:/Users/Talyah Greyling/Documents/1) Meesters/1) Data Science 871/Data_Science_Exam_23761067_GitHub) and then using fmxdat::make_project(ProjNam = "23761067"). 

#This code was used to create the question folders: 
location <- "C:/Users/Talyah Greyling/Documents/1) Meesters/1) Data Science 871/Data_Science_Exam_23761067_GitHub"
Texevier::create_template(directory = location, template_name = "Question1")
Texevier::create_template(directory = location, template_name = "Question2")
Texevier::create_template(directory = location, template_name = "Question3")
Texevier::create_template(directory = location, template_name = "Question4")
Texevier::create_template(directory = location, template_name = "Question5")

```

# DATA STORAGE: 

I unzipped the data folder provided (datsci.nfkatzke.com/PracData25.zip) and put each data file in its respective question folder's corresponding data file.
I then added '*data/' to my gitignore to prevent the data folders from committing to GitHub. 

I also stored the 'Practical_25.pdf' in my bin folder. 


# CODE USED FOR FIGURES AND TABLES: 

```{r}

# Housekeeping 
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.width = 6, fig.height = 5, fig.pos = 'H') # set chunk defaults 
rm(list = ls()) # clear environment 
gc() # garbage collection 
options(scipen = 999) #suppress scientific notation
options(dplyr.summarise.inform=F) # suppress messages

# Load neccessary libraries 
pacman::p_load(dplyr, ggplot2, tidyverse, stringr, purrr, lubridate, gt, fmsb, hrbrthemes) ############## update 

# Source in all functions: 
list.files('Question1/code/', full.names = T, recursive = T) %>% .[grepl('.R', .)] %>% as.list() %>% walk(~source(.))
list.files('Question2/code/', full.names = T, recursive = T) %>% .[grepl('.R', .)] %>% as.list() %>% walk(~source(.))
list.files('Question3/code/', full.names = T, recursive = T) %>% .[grepl('.R', .)] %>% as.list() %>% walk(~source(.))
list.files('Question4/code/', full.names = T, recursive = T) %>% .[grepl('.R', .)] %>% as.list() %>% walk(~source(.))
list.files('Question5/code/', full.names = T, recursive = T) %>% .[grepl('.R', .)] %>% as.list() %>% walk(~source(.))

```
\newpage
# QUESTION 1: Baby Names

## Load data 
```{r}
# Load data
location <- "Question1/data/US_Baby_names/" # set up directory path to data 
Baby_names <- read_rds(glue::glue("{location}Baby_Names_By_US_State.rds"))
Top_100_billboard <- read_rds(glue::glue("{location}charts.rds")) 
HBO_titles <- read_rds(glue::glue("{location}HBO_titles.rds"))
HBO_credits <- read_rds(glue::glue("{location}HBO_credits.rds")) 
```

## Process explained 
* Created the function *** that takes *** as an input and ***
* Created the function *** that takes *** as an input and ***
* I decided to look at naming trends nationally for the US since this was in line with the clients request to start by showing a time-series representation of the Spearman rank-correlation between each year’s 25 most popular boys’ and girls’ names
and that of the next 3 years and meant that substantial wrangling work was already done in this format. 
* To look at name popularity I decided to look at the top 5 nationally per year for boys and girls respectively, but upon time series visualisation saw that the persistence was too weak over the while time series to generate useful plots. I therefore decided to identify the top 10 names per decade and visualise those in a heatmap. 
* Lastly 

### Time-series representation of the Spearman rank-correlation
* I created a function 'top_n_names' that filters and ranks the baby names by year & gender (boys vs girls). 
* Thereafter I created a function 'calculate_correlations' that computes the Spearman Rank Correlation for a specific target year
* Then I created a function 'name_persistence' that applies this correlation calculation to all years such that the output can be visually represented. 
* Lastly, the function 'plot_name_persistence' uses geom_smooth to fit a series of lines that represent the rank-correlation over time  

#### Plot number 1 
```{r, warning =  FALSE, fig.align = 'center', fig.cap = "Persistence of National Top 25 Baby Names (1910-2014).\\label{Figure1}", fig.ext = 'png', fig.height = 5, fig.width = 10}
top_names <- Baby_names %>% 
  top_n_names(n = 25)
SpearCorr <- calculate_correlations(top_names, target_year = 2000, future_years = 3)
corr_results <- name_persistence(top_names, future_years = 3)
PersistPlot <- plot_name_persistence(corr_results = corr_results) 
PersistPlot

```

### Popular names 
* After this requested initial rank-correlation analysis it would be useful to see which names were the most popular 
* I created a function 'top_n_names_per_decade' that I use to determine to top 10 boy and girl names respectively per decade
* I then created a function 'popular_names_heatmap' that generates a heatmap to illustrate persistance of the top 10 names in each decade. 

#### Plot number 2
```{r, warning =  FALSE, fig.align = 'center', fig.cap = "Persistence of National Top 10 Baby Names per decade (1910-2014).\\label{Figure2}", fig.ext = 'png', fig.height = 20, fig.width = 10}
pop_names <- Baby_names %>% 
  top_n_names(n = 10)
decade_top_names <- top_n_names_per_decade(pop_names, top_n = 10)
PopNamesMap <- popular_names_heatmap(decade_top_names)
PopNamesMap

```

### Do movies determine popular names 
* TMDB stands for The Movie Database. The percentage is a score given to that movie or TV show by the database users on a 10-star scale. This can give you an idea of how other viewers feel about the show.
* Do people name babies after famous actors/ actresses who play popular movie characters? Define popular as a movie that scores above average on TMDB. 
* To explore this question I created a function 'get_popular_movies' that determines which movies got above average TMDB scores 
*

#### Plot 3 

### Issues encountered 
* I wrote a function to silently collate the rds files, but upon its creation saw that many of the columns then became unusable by containing an abundance of NAs, I decided to read in each file seperately by adapting the code provided in the assignment. 

\newpage
## Plots, graphs and tables reprinted

\newpage
# QUESTION 2: Music Taste  

## Load data 
```{r}
location <- "Question2/data/Coldplay_vs_Metallica/" # set up directory path to data 
df_metallica <- read_csv(glue::glue("{location}metallica.csv")) # read in csv file
df_coldplay <- read_csv(glue::glue("{location}Coldplay.csv")) 
df_spotify <- read_rds(glue::glue("{location}Broader_Spotify_Info.rds")) # read in rds file
df_billboard_100 <- read_rds(glue::glue("{location}charts.rds"))
```

## Process explained
* Wrote the introduction
* Wrote context for less avid listeners so that all readers of the report can understand its contents. 
    * Includes 2 references
* Created the function 'merge_datasets' that takes 'df_metallica' and  'df_coldplay' as an inputs and merges them to create a single data set, 'df_metcol'
* Created a list of words that indicate duplicates of a song that has already been recorded, 'duplicate_words'
* Created the function 'no_duplicate_words' that takes 'col_name' as an input and indicates if any duplicate words are present
* Used 'duplicate_words' and 'no_duplicate_words' to filter by the 'name' and 'album' columns in my data set
* Added a column 'month_year' in date format 
* Created the function 'get_stack_df' that takes 'df_metcol_unique' as an input and get summary statistics in long format that I can plot a stacked bar graph for both Coldplay and Metallica
* Created the function 'draw_table_summary' that takes 'df_metcol_unique' as an input and create a table containing basic summary stats for both Coldplay and Metallica
* Drew 'table_summary' and commented on the statistics in the section "Consistent Popularity, Divergent Paths"
* Created the function 'draw_stack' that takes 'df_stack' as an input and creates a stacked bar graph of summary statistics for both Coldplay and Metallica
* Drew 'stack_plot' and commented on the statistics in the section "Comparing Characteristics"
* Created the function 'draw_best_albums_table' that takes 'df_metcol_unique' as an input and creates a table of summary statistics for the best performing albums for both Coldplay and Metallica
* Drew 'table_best_albums' and commented on the statistics in the section "Comparing top albums"
* Wrote the conclusion

\newpage
## Plots, graphs and tables reprinted

\newpage
# QUESTION 3: Netflix 

## Load data 
```{r}
location <- "Question3/data/netflix/"
df_titles <- read_rds(glue::glue("{location}titles.rds"))
df_credits <- read_rds(glue::glue("{location}credits.rds"))
df_movies <-  read_csv(glue::glue("{location}netflix_movies.csv"))
```

## Process explained
* Wrote a short introduction with 1 reference 
* Created the function 'add_main_genre' that takes 'df_titles' as an input and identifies the main genre as the first entry in df_titles and stores it in a new column 'main_genre' 
* Created the function 'counts_genre' that takes 'df_titles' as an input and generates the genre distribution by counting the occurences in Netflix data
* Created the function 'get_top_n_genres' that takes 'df_titles' and 'n' as inputs and calculates the top n genres by average audience score on IMDB
* Created the function 'draw_genre_distribution' that takes 'genre_distribution' as an input and creates a bar plot of the distribution of genres on Netflix 
* Created the function 'draw_top_n_genres' that takes 'top_10_genres' as an input and creates a bar plot of the top 10 genres by average audience score on IMDB
* Created a duration data frame by combining 'df_titles' and 'df_movies'
* Created the function 'get_duration_distribution' that takes 'df_duration' as an input and generates the distribution of duration by summing occurrences in Netflix data in bins of 10 minutes 
* Created the function 'get_top_n_durations' that takes 'df_duration' as an input and calculates the top n durations by average audience score on IMDB
* Created the function 'draw_duration_distribution' that takes 'duration_distribution' as an input and creates a barplot of the distribution of durations on Netflix
* Created the function 'draw_top_n_durations' that takes 'top_10_durations' as an input and create a bar plot of the top 10 durations by average audience score on IMDB
* Created the function *** that takes *** as an input and ***
* Created the function *** that takes *** as an input and ***
* Drew 'genre_distribution_plot', 'top_n_genres_plot', 'duration_distribution_plot' and 'top_n_durations_plot' (as depicted below) 
* Discussed my findings in a brief summary 

\newpage
## Plots, graphs and tables reprinted 
```{r, fig.align = 'center', fig.cap = "Distribution of genres on Netflix", fig.ext = 'png', fig.height = 5, fig.width = 7}
# Data
df_titles <- add_main_genre(df_titles)
genre_distribution <- counts_genre(df_titles)

# Plot 
genre_distribution_plot <- draw_genre_distribution(genre_distribution)
genre_distribution_plot

```
```{r, fig.align = 'center', fig.cap = "Top genres according to audience", fig.ext = 'png', fig.height = 5, fig.width = 7}

# Data 
top_10_genres <- get_top_n_genres(df_titles, 10)

# Plot 
top_n_genres_plot <- draw_top_n_genres(top_10_genres)
top_n_genres_plot

```

```{r, fig.align = 'center', fig.cap = "Distribution of durations on Netflix", fig.ext = 'png', fig.height = 5, fig.width = 7}

# Data
df_duration <- left_join(df_titles, df_movies, by = "title") %>% 
    filter(!is.na(imdb_score)) %>%
    mutate(minutes = sub(" min", "", duration))
duration_distribution <- get_duration_distribution(df_duration)

# Plot  
duration_distribution_plot <- draw_duration_distribution(duration_distribution)
duration_distribution_plot

```

```{r, fig.align = 'center', fig.cap = "Top genres according to audience", fig.ext = 'png', fig.height = 5, fig.width = 7}

# Data 
top_10_durations <- get_top_n_durations(df_duration, 10)

# Plot
top_n_durations_plot <- draw_top_n_durations(top_10_durations)
top_n_durations_plot

```

\newpage
# QUESTION 4: Billionaires 

## Load data 
```{r}
df_billions <- "Question4/data/Billions/billionaires.csv" %>% read_csv_with_col_type()
```

## Process explained 
* Fixed inconsistent naming conventions in billionaires.csv
* Added a column 'Automatic Type' to Info file.xlsx
* Read through StackOverflow thread to resolve read_csv issue: decided to set the default column type as character & adjust the necessary columns to double and integer. 
* Created the function 'read_csv_with_col_type' that takes 'csv_file' as an input and then reads in a CSV document with its specific column types
* Wrote the introduction with 2 references  
* Wrote the exploration goals
* Created the functions 'counts_new_us' and 'counts_new_other' which take the data frame 'df_billions' as input and counts the amount of new billionaires & billionaires by inheritance per decade in the US and other countries respectively
* Created the function 'draw_first_claim' that takes 'df_us_billions' and 'input_title' as inputs to create a grouped bar chart of the counts of new vs. inherited wealth per decade 
* Drew 'us_plot_first_claim' (as depicted below) and discussed my findings
* Drew 'other_plot_first_claim' (as depicted below) and discussed my findings
* Created the function 'counts_software' that takes 'df_billions' and 'software_words' as inputs to count the amount of new billionaires through software vs. consumer services (per decade)
* Created the function 'draw_second_claim_software' that takes 'df_software_billions' as an input to create a lollipop plot of the amount of new billionaires through software vs. consumer services (per decade)
* Drew 'draw_second_claim_software' (as depicted below) and discussed my findings
* Wrote the conclusion

\newpage
## Plots, graphs and tables reprinted
```{r, fig.align = 'center', fig.cap = "New vs. inherited wealth per decade (US)", fig.ext = 'png', fig.height = 5, fig.width = 7}

# Data 
df_us_billions <- counts_new_us(df_billions)

#Plot
us_plot_first_claim <- draw_first_claim(df_us_billions, input_title = "Counts of new vs inherited wealth in the US per decade")
us_plot_first_claim

```

```{r, fig.align = 'center', fig.cap = "New vs. inherited wealth per decade (other countries)", fig.ext = 'png', fig.height = 5, fig.width = 7}

# Data 
df_other_billions <- counts_new_other(df_billions)

# Plot
other_plot_first_claim <- draw_first_claim(df_other_billions, input_title = "Counts of new vs inherited wealth in other countries per decade")
other_plot_first_claim

```

```{r, fig.align = 'center', fig.cap = "New Billionaires from Software vs Consumer Services (per decade)", fig.ext = 'png', fig.height = 5, fig.width = 12}

# Data 
df_software_billions <- counts_software(df_billions, c("software","technology"))

# Plot
software_plot_second_claim <- draw_second_claim_software(df_software_billions)
software_plot_second_claim
```
\newpage
# QUESTION 5: Health 

## Process explained 
* Read the question at 1am & thought it looked really nice
* Decided to use my remaining hours to iron out issues in my previous questions

\newpage
## Plots, graphs and tables reprinted
